{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4c08050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import tracemalloc\n",
    "import hdbscan\n",
    "from tslearn.clustering import KShape\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.cluster import AgglomerativeClustering, MiniBatchKMeans\n",
    "from tslearn.clustering import KShape\n",
    "from scipy.cluster.hierarchy import linkage as scipy_linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83f46af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_snippets_as_json(snippets, path):\n",
    "    serializable_snippets = [\n",
    "        {\n",
    "            'index': int(idx), \n",
    "            'subsequence': subseq.tolist() \n",
    "        }\n",
    "        for idx, subseq in snippets\n",
    "    ]\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(serializable_snippets, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0675cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(fig, path):\n",
    "    fig.savefig(path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a54e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(output_dir, snippets, metrics):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    save_snippets_as_json(snippets, os.path.join(output_dir, 'snippets.json'))\n",
    "\n",
    "    # Salva metrics\n",
    "    with open(os.path.join(output_dir, 'metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bdb04db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_number_list(s: str):\n",
    "    cleaned = s.replace('\\n', ',').replace(' ', '')\n",
    "    parts = [x for x in cleaned.split(',') if x]  \n",
    "    return [float(x) for x in parts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5cb42a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "\n",
    "base_path = '/home/guilherme-sales/insight_samsung/snippets/mixed_bag_eval/MixedBag'\n",
    "\n",
    "for file in os.listdir(base_path):\n",
    "    with open(os.path.join(base_path, file), 'r') as f:\n",
    "        lines = f.read()\n",
    "    dataset[file] = parse_number_list(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "544fb8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette_scores(k_values, scores):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(k_values, scores, marker='o')\n",
    "    plt.title('k vs Silhouette Score')\n",
    "    plt.xlabel('Número de Clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41374dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(Z, title='Dendrograma'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    dendrogram(Z)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a00a9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regime_bar(min_idx_per_segment, title='Regime Bar'):\n",
    "    plt.figure(figsize=(12, 1))\n",
    "    plt.imshow([min_idx_per_segment], aspect='auto', cmap='tab10')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Segment Index')\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee970a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_subsequences(segments_norm, num_clusters=None, method='kshape', linkage='ward', min_cluster_size=5, batch_size=100):\n",
    "    \"\"\"\n",
    "    Clusteriza subsequências usando KShape, Agglomerative, Hierarchical, HDBSCAN ou MiniBatchKMeans.\n",
    "\n",
    "    Parâmetros:\n",
    "    - segments_norm: np.array (n_samples, subseq_length)\n",
    "    - num_clusters: int (ignorado se HDBSCAN)\n",
    "    - method: 'kshape', 'agglomerative', 'hierarchical', 'hdbscan', 'minibatchkmeans'\n",
    "    - linkage: usado para aglomerativos\n",
    "    - min_cluster_size: usado para HDBSCAN\n",
    "    - batch_size: usado para MiniBatchKMeans\n",
    "\n",
    "    Retorna:\n",
    "    - labels: np.array de rótulos de clusters\n",
    "    - centroids: np.array de centroides\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'kshape':\n",
    "        kshape = KShape(n_clusters=num_clusters, random_state=0)\n",
    "        kshape.fit(segments_norm)\n",
    "        labels = kshape.labels_\n",
    "        centroids = kshape.cluster_centers_.squeeze()\n",
    "\n",
    "    elif method == 'agglomerative':\n",
    "        clustering = AgglomerativeClustering(n_clusters=num_clusters, linkage=linkage)\n",
    "        labels = clustering.fit_predict(segments_norm)\n",
    "        centroids = []\n",
    "        for i in range(num_clusters):\n",
    "            cluster_segs = segments_norm[labels == i]\n",
    "            if len(cluster_segs) == 0:\n",
    "                centroids.append(np.zeros(segments_norm.shape[1]))\n",
    "            else:\n",
    "                centroids.append(np.mean(cluster_segs, axis=0))\n",
    "        centroids = np.array(centroids)\n",
    "\n",
    "    elif method == 'hierarchical':\n",
    "        distance_matrix = pdist(segments_norm, metric='euclidean')\n",
    "        Z = scipy_linkage(distance_matrix, method=linkage)\n",
    "        labels = fcluster(Z, t=num_clusters, criterion='maxclust') - 1\n",
    "        centroids = []\n",
    "        for i in range(num_clusters):\n",
    "            cluster_segs = segments_norm[labels == i]\n",
    "            if len(cluster_segs) == 0:\n",
    "                centroids.append(np.zeros(segments_norm.shape[1]))\n",
    "            else:\n",
    "                centroids.append(np.mean(cluster_segs, axis=0))\n",
    "        centroids = np.array(centroids)\n",
    "\n",
    "    elif method == 'hdbscan':\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean')\n",
    "        labels = clusterer.fit_predict(segments_norm)\n",
    "        unique_labels = set(labels) - {-1}\n",
    "        centroids = []\n",
    "        for i in unique_labels:\n",
    "            cluster_segs = segments_norm[labels == i]\n",
    "            if len(cluster_segs) == 0:\n",
    "                centroids.append(np.zeros(segments_norm.shape[1]))\n",
    "            else:\n",
    "                centroids.append(np.mean(cluster_segs, axis=0))\n",
    "        centroids = np.array(centroids)\n",
    "\n",
    "    elif method == 'minibatchkmeans':\n",
    "        mbk = MiniBatchKMeans(n_clusters=num_clusters, batch_size=batch_size, random_state=0)\n",
    "        mbk.fit(segments_norm)\n",
    "        labels = mbk.labels_\n",
    "        centroids = mbk.cluster_centers_\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Método inválido. Escolha 'kshape', 'agglomerative', 'hierarchical', 'hdbscan' ou 'minibatchkmeans'.\")\n",
    "\n",
    "    return labels, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c9363c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_k(segments_norm, method='kshape', linkage='ward', k_range=(2, 10), plot=True):\n",
    "    best_score = -1\n",
    "    best_k = k_range[0]\n",
    "    k_values = []\n",
    "    scores = []\n",
    "\n",
    "    for k in range(k_range[0], k_range[1] + 1):\n",
    "        try:\n",
    "            labels, _ = clustering_subsequences(\n",
    "                segments_norm, \n",
    "                num_clusters=k, \n",
    "                method=method, \n",
    "                linkage=linkage\n",
    "            )\n",
    "            if len(set(labels)) < 2:\n",
    "                continue\n",
    "            score = silhouette_score(segments_norm, labels)\n",
    "            k_values.append(k)\n",
    "            scores.append(score)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_k = k\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if plot and k_values:\n",
    "        plot_silhouette_scores(k_values, scores)\n",
    "\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e563ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_snippets_clustering(ts, subseq_size, num_snippets=None, num_clusters=None, \n",
    "                              distance_metric='euclidean', weighted='only_profile',\n",
    "                              clustering_method='kshape', clustering_linkage='ward',\n",
    "                              min_cluster_size=5, batch_size=100,\n",
    "                              auto_k_selection=False, k_range=(2, 10)):\n",
    "    start = datetime.now()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    ts = np.array(ts, dtype=float)\n",
    "    segments_raw = np.array([ts[i:i + subseq_size] for i in range(0, len(ts) - subseq_size + 1)])\n",
    "    segments_norm = TimeSeriesScalerMeanVariance().fit_transform(segments_raw).squeeze()\n",
    "\n",
    "    # Seleção automática de k\n",
    "    if clustering_method not in ['hdbscan'] and auto_k_selection:\n",
    "        num_clusters = select_best_k(\n",
    "            segments_norm, \n",
    "            method=clustering_method, \n",
    "            linkage=clustering_linkage, \n",
    "            k_range=k_range,\n",
    "            plot=True\n",
    "        )\n",
    "        print(f\"[INFO] Melhor k selecionado: {num_clusters}\")\n",
    "\n",
    "    # Clustering\n",
    "    labels, centroids = clustering_subsequences(\n",
    "        segments_norm, \n",
    "        num_clusters=num_clusters, \n",
    "        method=clustering_method, \n",
    "        linkage=clustering_linkage, \n",
    "        min_cluster_size=min_cluster_size,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Dendrograma se hierarchical\n",
    "    if clustering_method == 'hierarchical':\n",
    "        distance_matrix = pdist(segments_norm, metric='euclidean')\n",
    "        Z = scipy_linkage(distance_matrix, method=clustering_linkage)\n",
    "        plot_dendrogram(Z)\n",
    "\n",
    "    # Ajuste para num_snippets\n",
    "    if num_snippets is None:\n",
    "        if clustering_method == 'hdbscan':\n",
    "            num_snippets = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        else:\n",
    "            num_snippets = num_clusters\n",
    "\n",
    "    medoides = []\n",
    "    cluster_sizes = []\n",
    "    cluster_densities = []\n",
    "\n",
    "    unique_clusters = set(labels) - {-1}  # remove ruído se hdbscan\n",
    "\n",
    "    for i, cluster_id in enumerate(unique_clusters):\n",
    "        cluster_idxs = np.where(labels == cluster_id)[0]\n",
    "        if len(cluster_idxs) == 0:\n",
    "            continue\n",
    "        segs_cluster = segments_norm[cluster_idxs]\n",
    "        centroide = centroids[i]\n",
    "\n",
    "        if distance_metric == 'euclidean':\n",
    "            dists = np.linalg.norm(segs_cluster - centroide, axis=1)\n",
    "        elif distance_metric == 'manhattan':\n",
    "            dists = np.sum(np.abs(segs_cluster - centroide), axis=1)\n",
    "        elif distance_metric == 'cosine':\n",
    "            dists = cosine_distances(segs_cluster, centroide.reshape(1, -1)).flatten()\n",
    "        else:\n",
    "            dists = np.linalg.norm(segs_cluster - centroide, axis=1)  # default euclidean\n",
    "\n",
    "        cluster_sizes.append(len(cluster_idxs))\n",
    "        cluster_densities.append(np.mean(dists))\n",
    "        medoid_local_idx = cluster_idxs[np.argmin(dists)]\n",
    "        medoides.append((medoid_local_idx, segments_norm[medoid_local_idx]))\n",
    "\n",
    "    mpdist_profile = np.full(len(segments_norm), np.inf)\n",
    "    all_profiles = []\n",
    "    match_count_coverages = []\n",
    "\n",
    "    for i, (medoid_idx, medoid) in enumerate(medoides):\n",
    "        if distance_metric == 'euclidean':\n",
    "            dists = np.linalg.norm(segments_norm - medoid, axis=1)\n",
    "        elif distance_metric == 'manhattan':\n",
    "            dists = np.sum(np.abs(segments_norm - medoid), axis=1)\n",
    "        elif distance_metric == 'cosine':\n",
    "            dists = cosine_distances(segments_norm, medoid.reshape(1, -1)).flatten()\n",
    "        else:\n",
    "            dists = np.linalg.norm(segments_norm - medoid, axis=1)\n",
    "\n",
    "        all_profiles.append(dists.copy())\n",
    "        threshold = np.max(dists) * 0.25\n",
    "        match_count_coverages.append(np.sum(dists <= threshold) / len(dists))\n",
    "\n",
    "        if weighted == 'size':\n",
    "            weight = cluster_sizes[i]\n",
    "        elif weighted == 'density':\n",
    "            weight = cluster_densities[i]\n",
    "        elif weighted == 'size_density':\n",
    "            weight = cluster_sizes[i] * cluster_densities[i]\n",
    "        else:\n",
    "            weight = 1\n",
    "\n",
    "        mpdist_profile = np.minimum(mpdist_profile, dists / weight)\n",
    "\n",
    "    all_profiles_array = np.array(all_profiles)\n",
    "    min_idx_per_segment = np.argmin(all_profiles_array, axis=0)\n",
    "    profile_area = [(min_idx_per_segment == i).sum() / len(min_idx_per_segment) for i in range(len(medoides))]\n",
    "    min_profile = np.min(all_profiles_array, axis=0)\n",
    "    cover_area = np.sum(min_profile)\n",
    "\n",
    "    top_idxs = np.argsort(mpdist_profile)[:num_snippets]\n",
    "    snippets = [(idx, segments_raw[idx]) for idx in top_idxs]\n",
    "\n",
    "    # Regime Bar\n",
    "    plot_regime_bar(min_idx_per_segment)\n",
    "\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "\n",
    "    metrics = {\n",
    "        'execution_time_sec': elapsed,\n",
    "        'memory_usage_mb': round(current / (1024 ** 2), 2),\n",
    "        'peak_memory_mb': round(peak / (1024 ** 2), 2)\n",
    "    }\n",
    "\n",
    "    return snippets, match_count_coverages, profile_area, cover_area, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_clusterings_for_series(series_name, series_list, subseq_size, k_range=(2, 10)):\n",
    "    \"\"\"\n",
    "    Executa clustering para várias séries temporais e métodos,\n",
    "    organizando os resultados por série e método.\n",
    "    \n",
    "    Parâmetros:\n",
    "    - series_list: lista de séries temporais (list of np.array)\n",
    "    - subseq_size: tamanho das subsequências\n",
    "    - k_range: intervalo para busca de k\n",
    "    \"\"\"\n",
    "    methods = ['agglomerative', 'hierarchical', 'hdbscan', 'minibatchkmeans']\n",
    "    base_dir = './resultados'\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    for serie_name, ts in zip(series_name, series_list):\n",
    "        serie_dir = os.path.join(base_dir, serie_name.replace('.txt', ''))\n",
    "        os.makedirs(serie_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n[INFO] Processando Série {serie_name.replace('.txt', '')}...\")\n",
    "\n",
    "        for method in methods:\n",
    "            print(f\"[INFO] Executando para método: {method.upper()}\")\n",
    "            output_dir = os.path.join(serie_dir, method)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Configurações padrão\n",
    "            kwargs = {\n",
    "                'ts': ts,\n",
    "                'subseq_size': subseq_size,\n",
    "                'clustering_method': method,\n",
    "                'auto_k_selection': method not in ['hdbscan'],\n",
    "                'k_range': k_range,\n",
    "                'num_snippets': 5,  \n",
    "            }\n",
    "            \n",
    "            if method == 'hdbscan':\n",
    "                kwargs['min_cluster_size'] = 5\n",
    "            if method == 'minibatchkmeans':\n",
    "                kwargs['batch_size'] = 50\n",
    "\n",
    "            plt.ioff()\n",
    "            original_plot = plt.show\n",
    "            plt.show = lambda: None\n",
    "\n",
    "            snippets, mcc, pa, ca, metrics = find_snippets_clustering(**kwargs)\n",
    "\n",
    "            # Salvar regime_bar\n",
    "            fig = plt.figure(figsize=(12, 1))\n",
    "            min_idx = np.arange(len(ts) - subseq_size + 1) % len(snippets)\n",
    "            plt.imshow([min_idx], aspect='auto', cmap='tab10')\n",
    "            plt.title('Regime Bar')\n",
    "            plt.xlabel('Segment Index')\n",
    "            plt.yticks([])\n",
    "            save_plot(fig, os.path.join(output_dir, 'regime_bar.png'))\n",
    "\n",
    "            # Salvar dendrograma se hierarchical\n",
    "            if method == 'hierarchical':\n",
    "                distance_matrix = pdist(TimeSeriesScalerMeanVariance().fit_transform(\n",
    "                    np.array([ts[i:i + subseq_size] for i in range(0, len(ts) - subseq_size + 1)])\n",
    "                ).squeeze(), metric='euclidean')\n",
    "                Z = scipy_linkage(distance_matrix, method='ward')\n",
    "                fig = plt.figure(figsize=(10, 5))\n",
    "                dendrogram(Z)\n",
    "                plt.title('Dendrograma')\n",
    "                save_plot(fig, os.path.join(output_dir, 'dendrograma.png'))\n",
    "\n",
    "            # Salvar silhouette se não for HDBSCAN\n",
    "            if method not in ['hdbscan']:\n",
    "                k_values = list(range(k_range[0], k_range[1]+1))\n",
    "                scores = []\n",
    "                for k in k_values:\n",
    "                    try:\n",
    "                        labels, _ = clustering_subsequences(\n",
    "                            TimeSeriesScalerMeanVariance().fit_transform(\n",
    "                                np.array([ts[i:i + subseq_size] for i in range(0, len(ts) - subseq_size + 1)])\n",
    "                            ).squeeze(), \n",
    "                            num_clusters=k, \n",
    "                            method=method\n",
    "                        )\n",
    "                        if len(set(labels)) > 1:\n",
    "                            score = silhouette_score(\n",
    "                                TimeSeriesScalerMeanVariance().fit_transform(\n",
    "                                    np.array([ts[i:i + subseq_size] for i in range(0, len(ts) - subseq_size + 1)])\n",
    "                                ).squeeze(), \n",
    "                                labels\n",
    "                            )\n",
    "                            scores.append(score)\n",
    "                        else:\n",
    "                            scores.append(0)\n",
    "                    except:\n",
    "                        scores.append(0)\n",
    "                fig = plt.figure(figsize=(8, 4))\n",
    "                plt.plot(k_values, scores, marker='o')\n",
    "                plt.title('k vs Silhouette Score')\n",
    "                plt.xlabel('Número de Clusters (k)')\n",
    "                plt.ylabel('Silhouette Score')\n",
    "                plt.grid(True)\n",
    "                save_plot(fig, os.path.join(output_dir, 'silhouette.png'))\n",
    "\n",
    "            # Salva snippets e métricas\n",
    "            save_results(output_dir, snippets, metrics)\n",
    "\n",
    "            plt.show = original_plot\n",
    "            plt.ion()\n",
    "\n",
    "            print(f\"[INFO] Resultados salvos em: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff36a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Processando Série EEGRat2_10_1000_75...\n",
      "[INFO] - Executando para método: AGGLOMERATIVE\n",
      "[INFO] Melhor k selecionado: 8\n",
      "[INFO]- Resultados salvos em: ./resultados/EEGRat2_10_1000_75/agglomerative\n",
      "[INFO] - Executando para método: HIERARCHICAL\n",
      "[INFO] Melhor k selecionado: 8\n",
      "[INFO]- Resultados salvos em: ./resultados/EEGRat2_10_1000_75/hierarchical\n",
      "[INFO] - Executando para método: HDBSCAN\n",
      "[INFO]- Resultados salvos em: ./resultados/EEGRat2_10_1000_75/hdbscan\n",
      "[INFO] - Executando para método: MINIBATCHKMEANS\n",
      "[INFO] Melhor k selecionado: 8\n",
      "[INFO]- Resultados salvos em: ./resultados/EEGRat2_10_1000_75/minibatchkmeans\n"
     ]
    }
   ],
   "source": [
    "run_all_clusterings_for_series(list(dataset.keys()), list(dataset.values()), subseq_size=150, k_range=(2, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snippets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
